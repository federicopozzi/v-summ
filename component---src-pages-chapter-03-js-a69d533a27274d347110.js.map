{"version":3,"sources":["webpack://v-summ/./src/assets/svg/arrowD.svg","webpack://v-summ/./src/assets/svg/x.svg","webpack://v-summ/./src/components/HeaderPages.js","webpack://v-summ/./src/components/Methodology.js","webpack://v-summ/./src/images/ch01/ch01.png","webpack://v-summ/./src/data/chapter.js","webpack://v-summ/./src/images/ch04/ch04.png","webpack://v-summ/./src/data/index.js","webpack://v-summ/./src/data/subchapter.js","webpack://v-summ/./src/images/ch01/subCh01.1.png","webpack://v-summ/./src/images/ch01/subCh01.2.png","webpack://v-summ/./src/pages/chapter03.js"],"names":["_path","_extends","Object","assign","target","i","arguments","length","source","key","prototype","hasOwnProperty","call","apply","this","SvgArrowD","props","width","height","fill","xmlns","d","stroke","strokeWidth","SvgX","HeaderPages","index","window","params","URLSearchParams","location","search","get","headerPages","data","primo","colore","coloreBordo","rq","className","to","style","transform","Methodology","methodology","aim","output","sfondo","capitolo2","capitolo3","capitolo4","viz","ch01","Chapter03"],"mappings":"yIAAIA,E,UAEJ,SAASC,IAA2Q,OAA9PA,EAAWC,OAAOC,QAAU,SAAUC,GAAU,IAAK,IAAIC,EAAI,EAAGA,EAAIC,UAAUC,OAAQF,IAAK,CAAE,IAAIG,EAASF,UAAUD,GAAI,IAAK,IAAII,KAAOD,EAAcN,OAAOQ,UAAUC,eAAeC,KAAKJ,EAAQC,KAAQL,EAAOK,GAAOD,EAAOC,IAAY,OAAOL,IAA2BS,MAAMC,KAAMR,WAIhT,SAASS,EAAUC,GACjB,OAAoB,gBAAoB,MAAOf,EAAS,CACtDgB,MAAO,GACPC,OAAQ,GACRC,KAAM,OACNC,MAAO,8BACNJ,GAAQhB,IAAUA,EAAqB,gBAAoB,OAAQ,CACpEqB,EAAG,wDACHC,OAAQ,OACRC,YAAa,Q,sECfbvB,E,UAEJ,SAASC,IAA2Q,OAA9PA,EAAWC,OAAOC,QAAU,SAAUC,GAAU,IAAK,IAAIC,EAAI,EAAGA,EAAIC,UAAUC,OAAQF,IAAK,CAAE,IAAIG,EAASF,UAAUD,GAAI,IAAK,IAAII,KAAOD,EAAcN,OAAOQ,UAAUC,eAAeC,KAAKJ,EAAQC,KAAQL,EAAOK,GAAOD,EAAOC,IAAY,OAAOL,IAA2BS,MAAMC,KAAMR,WAIhT,SAASkB,EAAKR,GACZ,OAAoB,gBAAoB,MAAOf,EAAS,CACtDgB,MAAO,IACPC,OAAQ,IACRC,KAAM,OACNC,MAAO,8BACNJ,GAAQhB,IAAUA,EAAqB,gBAAoB,OAAQ,CACpEqB,EAAG,mCACHC,OAAQ,OACRC,YAAa,Q,wHCRF,SAASE,IACtB,IAAIC,EAAQ,EACZ,GAAsB,oBAAXC,OAAwB,CACjC,IAAMC,EAAS,IAAIC,gBAAgBF,OAAOG,SAASC,QAEnDL,OAA0C,IAA3BE,EAAOI,IAAI,YAA8BJ,EAAOI,IAAI,YAAc,EAEnF,IAAMC,EAAcC,IAAKR,GAGvBS,EACEF,EADFE,MAAOC,EACLH,EADKG,OAAQC,EACbJ,EADaI,YAAaC,EAC1BL,EAD0BK,GAG9B,OACE,2BACE,uBAAKC,UAAU,8BACb,uBAAKA,UAAS,8BAAgCF,GAC5C,uBAAKE,UAAS,mCAAqCF,GACjD,uBAAKE,UAAS,wCAA0CF,GACtD,yBAAIF,IAEN,uBAAKI,UAAWH,GACd,qBAAGG,UAAU,WAAWD,KAG5B,uBAAKC,UAAU,SACb,gBAAC,KAAD,CAAMC,GAAG,KAAI,gBAAC,IAAD,CAAGC,MAAO,CAAEC,UAAW,oBAI1C,uBAAKH,UAAS,iDAAmDF,GAC/D,yCAEE,gBAAC,IAAD,CAAQI,MAAO,CAAEC,UAAW,qB,oGCnCvB,SAASC,IACtB,IAAIjB,EAAQ,EACZ,GAAsB,oBAAXC,OAAwB,CACjC,IAAMC,EAAS,IAAIC,gBAAgBF,OAAOG,SAASC,QAEnDL,OAA0C,IAA3BE,EAAOI,IAAI,YAA8BJ,EAAOI,IAAI,YAAc,EAEnF,IAAMY,EAAcV,IAAKR,GAEjBmB,EAA6BD,EAA7BC,IAAKC,EAAwBF,EAAxBE,OAAQT,EAAgBO,EAAhBP,YACrB,OACE,uBAAKE,UAAU,mBACb,uBAAKA,UAAS,yDAA2DF,GACvE,sCAEE,gBAAC,IAAD,CAAQI,MAAO,CAAEC,UAAW,kBAGhC,uBAAKH,UAAU,uBACb,uBAAKA,UAAU,SACb,wBAAMA,UAAU,kCAAhB,QAEF,uBAAKA,UAAU,UACb,qBAAGA,UAAU,QACVM,IAGL,uBAAKN,UAAU,SACb,wBAAMA,UAAU,kCAAhB,WAEF,uBAAKA,UAAU,UACb,qBAAGA,UAAU,QACV,IACAO,Q,kECtCb,MAAe,IAA0B,mDCGnCC,EAAS,8BAaFC,EAAY,CACvBb,MAAO,6FACPG,GAAI,uEACJO,IAAK,ynBACLT,OAAO,OAAQW,EACfV,YAAa,mBACbS,OAAQ,2TAGGG,EAAY,CACvBd,MAAO,sIACPG,GAAI,gGACJO,IAAK,knBACLT,OAAO,QAASW,EAChBV,YAAa,cACbS,OAAQ,uTAGGI,EAAY,CACvBf,MAAO,wGACPG,GAAI,wFACJa,ICrCa,IAA0B,mDDsCvCf,OAAO,UAAWW,EAClBV,YAAa,iBACbQ,IAAK,uZACLC,OAAQ,4LEpCV,GFDyB,CACvBX,MAAO,0FACPG,GAAI,wGACJa,IAAKC,EACLhB,OAAO,SAAUW,EACjBV,YAAa,gBACbQ,IAAK,miBAGLC,OAAQ,+TGVoB,CAC5BX,MAAO,4BACPG,GAAI,gFACJa,ICNa,IAA0B,wDDOvCf,OAAQ,oCACRC,YAAa,gBACbQ,IAAK,miBAGLC,OAAQ,+TAGoB,CAC5BX,MAAO,4BACPG,GAAI,gFACJa,IElBa,IAA0B,wDFmBvCf,OAAQ,oCACRC,YAAa,gBACbQ,IAAK,miBAGLC,OAAQ,+TDfRE,EACAC,EACAC,I,iHINa,SAASG,IACtB,OACE,2BACE,gBAAC,IAAD,MACA,gBAAC,IAAD","file":"component---src-pages-chapter-03-js-a69d533a27274d347110.js","sourcesContent":["var _path;\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nimport * as React from \"react\";\n\nfunction SvgArrowD(props) {\n  return /*#__PURE__*/React.createElement(\"svg\", _extends({\n    width: 31,\n    height: 24,\n    fill: \"none\",\n    xmlns: \"http://www.w3.org/2000/svg\"\n  }, props), _path || (_path = /*#__PURE__*/React.createElement(\"path\", {\n    d: \"M20 1v21M0 1h21M10.278 12.875l9.597 9.597 9.597-9.597\",\n    stroke: \"#000\",\n    strokeWidth: 2\n  })));\n}\n\nexport default \"data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzEiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAzMSAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTIwIDFMMjAgMjIiIHN0cm9rZT0iYmxhY2siIHN0cm9rZS13aWR0aD0iMiIvPgo8cGF0aCBkPSJNMCAxTDIxIDAuOTk5OTk5IiBzdHJva2U9ImJsYWNrIiBzdHJva2Utd2lkdGg9IjIiLz4KPHBhdGggZD0iTTEwLjI3ODMgMTIuODc0OUwxOS44NzUxIDIyLjQ3MTdMMjkuNDcxOSAxMi44NzQ5IiBzdHJva2U9ImJsYWNrIiBzdHJva2Utd2lkdGg9IjIiLz4KPC9zdmc+Cg==\";\nexport { SvgArrowD as ReactComponent };","var _path;\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nimport * as React from \"react\";\n\nfunction SvgX(props) {\n  return /*#__PURE__*/React.createElement(\"svg\", _extends({\n    width: 106,\n    height: 134,\n    fill: \"none\",\n    xmlns: \"http://www.w3.org/2000/svg\"\n  }, props), _path || (_path = /*#__PURE__*/React.createElement(\"path\", {\n    d: \"M1 133L104.314 1M4.525 1L105 133\",\n    stroke: \"#000\",\n    strokeWidth: 2\n  })));\n}\n\nexport default \"data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTA2IiBoZWlnaHQ9IjEzNCIgdmlld0JveD0iMCAwIDEwNiAxMzQiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CjxwYXRoIGQ9Ik0xIDEzM0wxMDQuMzE0IDEiIHN0cm9rZT0iYmxhY2siIHN0cm9rZS13aWR0aD0iMiIvPgo8cGF0aCBkPSJNNC41MjU0MyAwLjk5OTk5NUwxMDUgMTMzIiBzdHJva2U9ImJsYWNrIiBzdHJva2Utd2lkdGg9IjIiLz4KPC9zdmc+Cg==\";\nexport { SvgX as ReactComponent };","import React from 'react';\nimport { ReactComponent as X } from '../assets/svg/x.svg';\nimport '../scss/style.scss';\nimport { ReactComponent as ArrowD } from '../assets/svg/arrowD.svg';\nimport data from '../data';\nimport Link from 'gatsby-link';\n\nexport default function HeaderPages() {\n  let index = 0;\n  if (typeof window !== 'undefined') {\n    const params = new URLSearchParams(window.location.search);\n\n    index = typeof params.get('selected') !== 'undefined' ? params.get('selected') : 0;\n  }\n  const headerPages = data[index];\n\n  const {\n    primo, colore, coloreBordo, rq,\n  } = headerPages;\n\n  return (\n    <div>\n      <div className=\"container-fluid bg-primary\">\n        <div className={`row border-bottom border-1 ${coloreBordo}`}>\n          <div className={`col-11 fs-5 border-end border-2 ${coloreBordo}`}>\n            <div className={`row border-bottom py-3 px-3 border-2 ${coloreBordo}`}>\n              <p>{primo}</p>\n            </div>\n            <div className={colore}>\n              <p className=\"fw-bold\">{rq}</p>\n            </div>\n          </div>\n          <div className=\"col-1\">\n            <Link to=\"/\"><X style={{ transform: 'scale(0.7)' }} /></Link>\n          </div>\n        </div>\n      </div>\n      <div className={`row border-bottom border-2 bg-light px-3 py-1 ${coloreBordo}`}>\n        <p>\n          VISUALIZATION\n          <ArrowD style={{ transform: 'scale(0.6)' }} />\n\n        </p>\n      </div>\n\n    </div>\n  );\n}\n","import React from 'react';\nimport '../scss/style.scss';\nimport data from '../data';\nimport { ReactComponent as ArrowD } from '../assets/svg/arrowD.svg';\n\nexport default function Methodology() {\n  let index = 0;\n  if (typeof window !== 'undefined') {\n    const params = new URLSearchParams(window.location.search);\n\n    index = typeof params.get('selected') !== 'undefined' ? params.get('selected') : 0;\n  }\n  const methodology = data[index];\n\n  const { aim, output, coloreBordo } = methodology;\n  return (\n    <div className=\"container-fluid\">\n      <div className={`row border-end border-bottom border-top bg-light py-1 ${coloreBordo}`}>\n        <p>\n          METODOLOGY\n          <ArrowD style={{ transform: 'scale(0.6)' }} />\n        </p>\n      </div>\n      <div className=\"row bg-primary py-4\">\n        <div className=\"col-2\">\n          <span className=\"fs-2 text-decoration-underline\">aim</span>\n        </div>\n        <div className=\"col-10\">\n          <p className=\"fs-2\">\n            {aim}\n          </p>\n        </div>\n        <div className=\"col-2\">\n          <span className=\"fs-2 text-decoration-underline\">output</span>\n        </div>\n        <div className=\"col-10\">\n          <p className=\"fs-2\">\n            {' '}\n            {output}\n          </p>\n        </div>\n      </div>\n    </div>\n  );\n}\n","export default __webpack_public_path__ + \"static/ch01-5ab445226241d04e534a9a72a0d9f795.png\";","import ch01 from '../images/ch01/ch01.png';\nimport ch04 from '../images/ch04/ch04.png';\n\nconst sfondo = 'row border-danger py-3 px-3';\nexport const capitolo1 = {\n  primo: 'Identifying and visualising the main themes emerging from a video collection of videos.',\n  rq: 'RQ: Which are the main themes (based on number of scenes) in the Amazon Fires related YouTube videos?',\n  viz: ch01,\n  colore: `verde ${sfondo}`,\n  coloreBordo: 'border-danger',\n  aim: 'This method aims to identify which are the main themes emerging within\\n'\n      + '              a collection of videos. Frame extraction for this purpose is based on scene change detection, so that the images to be analysed are only taken once\\n'\n      + '              and there are no duplicates due to scene length. The layout used to arrange the frames according to their visual similarity is offered by Pixplot, which uses UMAP projection, a dimensionality reduction algorithm, specifically designed for visualising complex data in low dimensions (2D or 3D).\\n',\n  output: 'The final visualisation is a clusterisation of frames sorted by visual similarity that allows the identification of predominant thematic clusters within the analysed video collection. The thematic annotations of the visualisation were drawn following the boundaries identified by the original Pixplot visualisation.',\n};\n\nexport const capitolo2 = {\n  primo: 'Visualising main themes in each video to compare the content of two collections of videos.',\n  rq: 'RQ: How are themes distributed in each Amazon Fires related Youtube?',\n  aim: 'In this type of analysis, we move from an overview of the frames in the whole collection to a view of the frames for each video. The aim is to be able to compare the contents of two collections of videos based on the thematic clusters found in each video. Frame extraction for this purpose is based on scene change detection, so that the images to be analysed are only taken once and there are no duplicates due to scene length. All frames of a video are analysed with Pixplot and arranged in a UMAP grid based on their visual similarity. The different thematic clusters in each grid are highlighted with colour areas using Figma.',\n  colore: `blu ${sfondo}`,\n  coloreBordo: 'border-secondary',\n  output: 'The final visualisation consists of a series of matrices representing the videos, where the thematic components that are discussed in the video are represented with different colours.. This type of visualization allows a summary comparison of the thematic contents among the videos of two different collections.',\n};\n\nexport const capitolo3 = {\n  primo: 'Detecting and visualising the amount of human figures within one or more collections of videos and recognise the different formats.',\n  rq: 'RQ: How relevant is the presence of human figures in the Amazon Fires related YouTube videos?',\n  aim: 'This type of analysis can help to recognise different formats where human figures are particularly relevant, such as interviews or news anchors. Applying this method to two different video collections helps us to compare them in terms of the presence of human figures and types of formats. The model used for the extraction of human figures from each frame is DeepLabv3. To perform this type of analysis it is necessary to extract the frames at regular time intervals so as to also record the length of the scenes in which human figures are present and preserve the original length of each video in the final visualisation.',\n  colore: `rosa ${sfondo}`,\n  coloreBordo: 'border-info',\n  output: 'The final visualisation is made up of the 20 frame grids of the videos, divided by collection, and allows a direct comparison in terms of the presence of human figures and video formats (news anchors, interviews, image montage). it is possible to explore the videos more closely by selecting one collection.',\n};\n\nexport const capitolo4 = {\n  primo: 'Recognising the main humans faces in a collection of videos and visually quantifying  their presence.',\n  rq: 'RQ: Which are the most recognisable faces in the Amazon Fires related Youtube videos?',\n  viz: ch04,\n  colore: `giallo ${sfondo}`,\n  coloreBordo: 'border-warning',\n  aim: 'This type of analysis allows us to recognise the faces of human figures and to recognise those most present within a collection of videos. The segmentation model used on RunwayML removes the background from images featuring people. Frame extraction for this purpose is based on scene change detection, so that the images to be analysed are only taken once and there are no duplicates due to scene length.',\n  output: 'The final visualisation is a catalogue of faces of human figures, resized by the amount of presence in the analysed videos, and clustered by type (e.g. politicians, bloggers, natives).',\n};\n","export default __webpack_public_path__ + \"static/ch04-43772c4a54a0eb1f3ae252df7208a1d8.png\";","import {\n  capitolo1, capitolo2, capitolo3, capitolo4,\n} from './chapter';\nimport { sottocapitolo1, sottocapitolo2 } from './subchapter';\n\nexport default [\n  capitolo1,\n  sottocapitolo1,\n  sottocapitolo2,\n  capitolo2,\n  capitolo3,\n  capitolo4,\n\n];\n","import subCh01 from '../images/ch01/subCh01.1.png';\nimport subCh02 from '../images/ch01/subCh01.2.png';\n\nexport const sottocapitolo1 = {\n  primo: 'sottocapitolo1 of videos.',\n  rq: 'RQ: Which are the main themes dvsved the Amazon Fires related YouTube videos?',\n  viz: subCh01,\n  colore: 'row border-danger py-3 px-3 verde',\n  coloreBordo: 'border-danger',\n  aim: 'This method aims to identify which are the main themes emerging within\\n'\n      + '              a collection of videos. Frame extraction for this purpose is based on scene change detection, so that the images to be analysed are only taken once\\n'\n      + '              and there are no duplicates due to scene length. The layout used to arrange the frames according to their visual similarity is offered by Pixplot, which uses UMAP projection, a dimensionality reduction algorithm, specifically designed for visualising complex data in low dimensions (2D or 3D).\\n',\n  output: 'The final visualisation is a clusterization of frames sorted by visual similarity that allows the identification of predominant thematic clusters within the analysed video collection. The thematic annotations of the visualisation were drawn following the boundaries identified by the original Pixplot visualisation.',\n};\n\nexport const sottocapitolo2 = {\n  primo: 'sottocapitolo2 of videos.',\n  rq: 'RQ: Which are the main themes dvsved the Amazon Fires related YouTube videos?',\n  viz: subCh02,\n  colore: 'row border-danger py-3 px-3 verde',\n  coloreBordo: 'border-danger',\n  aim: 'This method aims to identify which are the main themes emerging within\\n'\n      + '              a collection of videos. Frame extraction for this purpose is based on scene change detection, so that the images to be analysed are only taken once\\n'\n      + '              and there are no duplicates due to scene length. The layout used to arrange the frames according to their visual similarity is offered by Pixplot, which uses UMAP projection, a dimensionality reduction algorithm, specifically designed for visualising complex data in low dimensions (2D or 3D).\\n',\n  output: 'The final visualisation is a clusterization of frames sorted by visual similarity that allows the identification of predominant thematic clusters within the analysed video collection. The thematic annotations of the visualisation were drawn following the boundaries identified by the original Pixplot visualisation.',\n};\n","export default __webpack_public_path__ + \"static/subCh01.1-0c95ef32543140b9dc69d9abd50a338c.png\";","export default __webpack_public_path__ + \"static/subCh01.2-f96af91fd000667c9f43eabc20982be2.png\";","import React from 'react';\nimport HeaderPages from '../components/HeaderPages';\nimport '../scss/style.scss';\nimport Methodology from '../components/Methodology';\n\nexport default function Chapter03() {\n  return (\n    <div>\n      <HeaderPages />\n      <Methodology />\n    </div>\n  );\n}\n"],"sourceRoot":""}